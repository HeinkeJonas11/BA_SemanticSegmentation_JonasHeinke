{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"P4_train_models.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6mOHbd3IvxA-"},"source":["# P4_neural-odes-segmentation-master_CRAG"]},{"cell_type":"markdown","metadata":{"id":"Sf9AcVhTj6zG"},"source":["## Google Colab"]},{"cell_type":"code","metadata":{"id":"vqxfFGN2v2dA"},"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvZxMhVLv5s6"},"source":["cd drive/MyDrive/BA_SemanticSegmentation_JonasHeinke/___P4_neural-odes-segmentation-master_CRAG/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHtkDk5ov56y"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2PynbQhv6ID"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRg7QhAvkgV_"},"source":["### Installation des Moduls torchdiffeq"]},{"cell_type":"code","metadata":{"id":"rO4X1gNbyPsq"},"source":["!pip install git+https://github.com/rtqichen/torchdiffeq"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wdngHuTRkAZ7"},"source":["## Change these flags to train a specific model"]},{"cell_type":"code","metadata":{"id":"dItUCsiTvxA_"},"source":["TRAIN_RESNET = False\n","TRAIN_UNODE = False\n","TRAIN_UNET = True\n","\n","def get_title():\n","    if TRAIN_UNODE: return 'U-NODE'\n","    elif TRAIN_RESNET: return 'RESNET'\n","    elif TRAIN_UNET: return 'UNET'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4TzKO5UvxA_"},"source":["---"]},{"cell_type":"code","metadata":{"id":"a2aL4J9GvxA_"},"source":["import os\n","import glob\n","import random\n","\n","import torch\n","import torch.utils.data\n","\n","import PIL\n","import skimage.measure\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm, tqdm_notebook\n","\n","%matplotlib inline\n","\n","from models import ConvODEUNet, ConvResUNet, ODEBlock, Unet\n","#from dataloader import GLaSDataLoader\n","from dataloader_crag import Crag_DataLoader\n","from train_utils import plot_losses\n","\n","from IPython.display import clear_output\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KYxCZr5jlTwS"},"source":["## Datenset laden\n","- Dieser Teil unterscheidet sich in Bezug zum Original, da eine Anpassung an das zu ladende Datenset CRAG_v2 erfolgt."]},{"cell_type":"markdown","metadata":{"id":"ntSzt-5pvxA_"},"source":["#### Download the filnames of dataset\n","\n","MILD-Net: \"Colorectal Adenocarcinoma Gland (CRAG) Dataset\"\n","\n","https://warwick.ac.uk/services/its/intranet/projects/webdev/sandbox/juliemoreton/research-copy/tia/data/mildnet\n","\n","Datenset zum Herunterladen:\n","\n","https://drive.google.com/u/0/uc?id=1p3dZXpgeA1IcGO6vXhStbVLMku-fZTmQ&export=download"]},{"cell_type":"code","metadata":{"id":"aLkJJvJTvxA_"},"source":["if not os.path.exists('CRAG_v2'):\n","    print('Bitte laden sie das Datenset in das Projektverzeichnis!')\n","    print('Das Verzeichnis lautet \"CRAG_v2\".')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8Gfu6y_vxA_"},"source":["from jh_path import Path as PATH   # Pfade und Dateinamen\n","path=PATH() # Instanz der Klasse für Methodenaufruf erforderlich\n","\n","path_images=PATH.dataset / 'train/Images/'\n","path_targets=PATH.dataset / 'train/Annotation/'\n","# input and target files\n","filenames_inputs  =path.get_filenames(path=path_images , dateifilter= '*.png')\n","filenames_targets =path.get_filenames(path=path_targets ,dateifilter='*.png')\n","# sicherstellen, dass beide Listen die gleiche Länge besitzen.\n","print('Anzahl der Bilder      : ', len(filenames_inputs))\n","print('Anzahl der Annotationen: ', len(filenames_targets))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79hvZUEvvxBA"},"source":["## Define datasets"]},{"cell_type":"code","metadata":{"id":"u5pv-roEvxBA"},"source":["\n","torch.manual_seed(0)\n","\n","val_set_idx = torch.LongTensor(10).random_(0, len(filenames_inputs)) #+\n","train_set_idx = torch.arange(0, len(filenames_inputs)) #+\n","\n","overlapping = (train_set_idx[..., None] == val_set_idx).any(-1)\n","train_set_idx = torch.masked_select(train_set_idx, ~overlapping)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZxT8C5hmaTp"},"source":["## Größe der Eingangsbild-Masken-Paare vereinbaren\n","- Verhältnis zwischen Höhe zu Weite h/w=1 "]},{"cell_type":"code","metadata":{"id":"TXShGevBvxBA"},"source":["\n","h=512 # 1024 #752 #758  #752 # orginal 1516\n","w=512 # 1024 #752 #754  #752  # orginal 1509\n","\n","trainset = Crag_DataLoader(filenames_inputs,filenames_targets,(h, w), dataset_repeat=1, images=train_set_idx)\n","valset = Crag_DataLoader(filenames_inputs,filenames_targets,(h, w), dataset_repeat=1, images=val_set_idx, validation=True)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=4)\n","valloader = torch.utils.data.DataLoader(valset, batch_size=1, shuffle=False, num_workers=4)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WA9ieMcevxBA"},"source":["# Plotting train data"]},{"cell_type":"code","metadata":{"id":"U8fwo7s5vxBA"},"source":["fig, ax = plt.subplots(nrows=5, ncols=6, figsize=(24, 15))\n","\n","for y in range(5): # y ist idx einer Stichprobe\n","    for x in range(3):  \n","        sample = trainset[y]\n","        ax[y, x * 2].imshow(sample[0].numpy().transpose(1,2,0))\n","        ax[y, x * 2 + 1].imshow(sample[1][0])\n","        ax[y, x * 2].axis('off')\n","        ax[y, x * 2 + 1].axis('off')\n","\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eW7YXVJzvxBB"},"source":["fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24, 15))\n","\n","sample = trainset[0]\n","ax[1].imshow(sample[1][0].numpy())\n","ax[2].imshow(sample[1].sum(dim=0))\n","ax[0].imshow(sample[0].numpy().transpose(1,2,0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80fNjfxhvxBB"},"source":["# Plotting validation data"]},{"cell_type":"code","metadata":{"id":"Ntz5MfGmvxBB"},"source":["fig, ax = plt.subplots(nrows=5, ncols=6, figsize=(24, 15))\n","\n","for y in range(5):\n","    for x in range(3):\n","        sample = valset[y]\n","        ax[y, x * 2].imshow(sample[0].numpy().transpose(1,2,0))\n","        ax[y, x * 2 + 1].imshow(sample[1][1])\n","        ax[y, x * 2].axis('off')\n","        ax[y, x * 2 + 1].axis('off')\n","\n","plt.show(); "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzYMr1MSvxBB"},"source":["# Define network"]},{"cell_type":"code","metadata":{"id":"Sh1Ys5bLvxBB"},"source":["device = torch.device('cuda')\n","\n","if TRAIN_UNODE:\n","    net = ConvODEUNet(num_filters=16, output_dim=2, time_dependent=True, \n","                      non_linearity='lrelu', adjoint=True, tol=1e-3)\n","    net.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wDXI-GsvxBB"},"source":["if TRAIN_RESNET:\n","    net = ConvResUNet(num_filters=16, output_dim=2, non_linearity='lrelu')\n","    net.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpYXfADPvxBB"},"source":["if TRAIN_UNET:\n","    net = Unet(depth=5, num_filters=64, output_dim=2).cuda()\n","    net.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2M2E1GLyvxBC"},"source":["---"]},{"cell_type":"code","metadata":{"id":"WH2tWHWrvxBC"},"source":["for m in net.modules():\n","    if isinstance(m, torch.nn.Conv2d):\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        torch.nn.init.constant_(m.bias, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbKY7q72vxBC"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cH5bxLwrvxBC"},"source":["count_parameters(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C26A7YJTvxBC"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"kd5X8SBavxBC"},"source":["criterion = torch.nn.BCEWithLogitsLoss()\n","val_criterion = torch.nn.BCEWithLogitsLoss()\n","\n","if TRAIN_UNET:\n","    cross_entropy = torch.nn.BCEWithLogitsLoss()\n","\n","    def criterion(conf, labels):\n","        out_shape = conf.shape[2:4]\n","        label_shape = labels.shape[2:4]\n","\n","        w = (label_shape[1] - out_shape[1]) // 2\n","        h = (label_shape[1] - out_shape[1]) // 2\n","        dh, dw = out_shape[0:2]\n","\n","        conf_loss_ce = cross_entropy(conf, labels[:, :, h:h+dh, w:w+dw])\n","\n","        return conf_loss_ce"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHyXg0eYvxBD"},"source":["optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5hko9SdvxBD"},"source":["torch.backends.cudnn.benchmark = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvIe1FUgvxBD"},"source":["losses = []\n","val_losses = []\n","nfe = [[],[],[],[],[],[],[],[],[]] if TRAIN_UNODE else None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNi_u4vAvxBD"},"source":["accumulate_batch =8  # 8  # mini-batch size by gradient accumulation\n","accumulated = 0\n","\n","if TRAIN_RESNET: filename = 'best_border_resnet_model.pt'\n","elif TRAIN_UNODE: filename = 'best_border_unode_model.pt'\n","elif TRAIN_UNET: filename = 'best_border_unet_model.pt'\n","\n","def run(lr=1e-3, epochs=100):\n","    accumulated = 0\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    for epoch in range(epochs):\n","        \n","        # training loop with gradient accumulation\n","        running_loss = 0.0\n","        optimizer.zero_grad()\n","        for data in tqdm(trainloader):\n","            inputs, labels = data[0].cuda(), data[1].cuda()\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels) / accumulate_batch\n","            loss.backward()\n","            accumulated += 1\n","            if accumulated == accumulate_batch:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","                accumulated = 0\n","\n","            running_loss += loss.item() * accumulate_batch\n","\n","        losses.append(running_loss / len(trainloader))\n","        \n","        # validation loop\n","        with torch.no_grad():\n","            running_loss = 0.0\n","            for data in valloader:\n","                inputs, labels = data[0].cuda(), data[1].cuda()\n","                outputs = net(inputs)\n","                loss = criterion(outputs, labels)\n","                running_loss += loss.item()\n","\n","            val_losses.append(running_loss / len(valloader))\n","            # Bedingung zum Speichern des Modells\n","            if np.argmin(val_losses) == len(val_losses) - 1 and loss < 0.4:\n","                torch.save(net, filename)\n","                #------Protokoll--------------------------\n","                protokolldatei = open('_protokoll.txt','a') #+\n","                protokolldatei.write('---------------------------------------------\\n')  #+\n","                protokolldatei.write(f'Speicherung des Modells nach {epoch} Epochen, loss: {loss}\\n') #+\n","                protokolldatei.close() #+\n","                #-------------------------------------\n","                \n","            clear_output(wait=True)\n","            plot_losses(inputs, outputs, losses, val_losses, get_title(), nfe, net=net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sJCcPFhvxBD"},"source":["if TRAIN_UNODE or TRAIN_RESNET: lr = 1e-3 \n","else: lr = 1e-4\n","\n","run(lr, 200 - len(losses))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cm5wwtnkvxBE"},"source":["## Calculate results"]},{"cell_type":"code","metadata":{"id":"fsFs5B29vxBE"},"source":["# load best model\n","net = torch.load(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0s1jge8vxBE"},"source":["with torch.no_grad():\n","    running_loss = 0.0\n","    for data in tqdm(valloader):\n","        inputs, labels = data[0].cuda(), data[1].cuda()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        running_loss += loss.item()\n","\n","    print(\"Check validation loss:\", running_loss / len(valloader))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fol4epEvxBE"},"source":["## Visualize results on validation set"]},{"cell_type":"code","metadata":{"id":"EuhifPtwvxBE"},"source":["#+ from inference_utils import inference_image, postprocess\n","from inference_utils_crag import inference_image, postprocess #+\n","import numpy as np              #+\n","import matplotlib.pyplot as plt #*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iiEjH_FAvxBE"},"source":["fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(4*3,3*5))\n","\n","ax[0, 0].set_title('Image')\n","ax[0, 1].set_title('Ground-truth')\n","ax[0, 2].set_title(get_title())\n","\n","for col in range(3):\n","    for row in range(5):\n","        index = val_set_idx[row]\n","        print(f'idx: {index}, ', end='')\n","        image = PIL.Image.open(path_images /  f'train_{index}.png')\n","        gt = PIL.Image.open(path_targets / f'train_{index}.png')\n","        \n","        with torch.no_grad():\n","            #? result, input_image = inference_image(net, image, shouldpad=TRAIN_UNET)\n","            result, input_image  = inference_image(net, image, shouldpad=TRAIN_UNET)\n","            result = postprocess(result, gt)\n","        if col == 0:\n","            ax[row, col].imshow(image)\n","        elif col == 1:\n","            ax[row, col].imshow(np.array(gt) > 0)\n","        else:\n","            ax[row, col].imshow(image)\n","            ax[row, col].imshow(result, alpha=0.5)\n","                \n","        ax[row, col].set_axis_off()\n","        \n","\n","\n","plt.show(); "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGN5meRsvxBE"},"source":["# Calculate metrics on test set"]},{"cell_type":"code","metadata":{"id":"hH9Uotx8vxBE"},"source":["from metrics import ObjectDice, ObjectHausdorff, F1score\n","import torch\n","import numpy as np\n","import PIL\n","import skimage.measure\n","from tqdm import tqdm, tqdm_notebook\n","from inference_utils_crag import inference_image, postprocess\n","import matplotlib.pyplot as plt\n","########################################################\n","from img_array_transform_jh import ArrayTransform as TRANSFORM\n","from _path import Path as PATH   # Pfade und Dateinamen\n","path=PATH() # Instanz der Klasse für Methodenaufruf erforderlich"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rNuWDX9vxBF"},"source":["TEST_RESNET = False\n","TEST_UNODE = False\n","TEST_UNET = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7O09nRBvxBF"},"source":["if TEST_UNODE: net = torch.load('best_border_unode_model.pt')\n","elif TEST_RESNET: net = torch.load('best_border_resnet_model.pt')\n","elif TEST_UNET: net = torch.load('best_border_unet_model.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2InFo2e1vxBF"},"source":["path_testimages=PATH.dataset / 'valid/Images/'\n","path_testtargets=PATH.dataset / 'valid/Annotation/'\n","\n","filenames_testinputs  =path.get_filenames(path=path_testimages , dateifilter= '*.png')\n","filenames_testtargets =path.get_filenames(path=path_testtargets ,dateifilter='*.png')\n","\n","print('Anzahl der Bilder      : ', len(filenames_testinputs))\n","print('Anzahl der Annotationen: ', len(filenames_testtargets))\n","\n","anzahl_testimages=len(filenames_testinputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gvnxp7xhvxBG"},"source":["### Identnummernbezogene Bewertung\n","[ Id0 - Hintergrund, Durchnummerierung der Drüsen je maske von Id=1, 2 , 3 , ..., \n","\n","- Anpassungen, da nur ein Testset\n","- Protokoll zum Abspeichern der Ergebnisse"]},{"cell_type":"code","metadata":{"id":"i86QbIuuvxBG"},"source":["\n","dice, hausdorff, f1, dice_full = 0, 0, 0, 0\n","\n","if TEST_UNODE: folder = 'results_unode'\n","elif TEST_UNET: folder = 'results_unet'\n","elif TEST_RESNET: folder = 'results_resnet'\n","    \n","names = []\n","i_error=0\n","anzahl=0\n","\n","for index in np.arange(1, anzahl_testimages+1):\n","    names.append(f'test_{index}.png')\n","  \n"," \n","for i, fname in tqdm_notebook(enumerate(names), total=anzahl_testimages):\n","    # tqdm.notebook.tqdm \n","    # print(f'idx: {i}, ', end='') #\n","    image = PIL.Image.open(path_testimages /  fname)\n","    gt = PIL.Image.open(path_testtargets / fname)\n","    \n","    result, resized = inference_image(net, image, shouldpad=TEST_UNET)\n","    result = postprocess(result, gt)\n","    \n","    \n","    gt = skimage.measure.label(np.array(gt))\n","    \n","\n","    #-# f1_img, hausdorff_img, dice_img =0, 0, 0\n","    try:\n","\n","        f1_img = F1score(result, gt)\n","        hausdorff_img = ObjectHausdorff(result, gt)\n","        dice_img = ObjectDice(result, gt)\n","        \n","        f1 += f1_img\n","        hausdorff += hausdorff_img\n","        dice += dice_img\n","        print(i,', ', fname,' : ', f1_img, hausdorff_img, dice_img)\n","        anzahl +=1\n","        \n","        #------Protokoll--------------------------\n","        protokolldatei = open('_protokoll.txt','a') #+\n","        protokolldatei.write('---------------------------------------------\\n')  #+\n","        protokolldatei.write(f'i: {i}: filename: {fname}, f1_img: {f1_img}, hausdorff: {hausdorff_img}, dice_img: {dice_img} \\n') #+\n","        #protokolldatei.write(f'image.shape: {image.shape}, gt.shape: {gt.shape}, result.shape: {result.shape} \\n') #+\n","        protokolldatei.close() #+\n","        #-------------------------------------\n","\n","    except:\n","        i_error +=1\n","        print('Error: ',i_error, 'Zyklus: ', i, 'Dateiname: ', fname)\n","    \n","\n","print('--Mittelwerte, Drüsen- bzw. Identnummernbezogen------------------------------------')\n","print('ObjectDice:', dice / anzahl )\n","print('Hausdorff:', hausdorff / anzahl)\n","print('F1:', f1 / anzahl )\n","print('Anzahl io.: ', anzahl)\n","print('Errors: ', i_error)\n"],"execution_count":null,"outputs":[]}]}